{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad1f03a",
   "metadata": {},
   "source": [
    "# Objectives: \n",
    "\n",
    "This is Scikit-learn Part I. This notebook mainly focus on how to use scikit-learn to deal with dataset. You will learn: \n",
    "\n",
    "1. standardization \n",
    "2. normalization \n",
    "3. encoding categorical features\n",
    "4. filling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167a827",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "SciKit-Learn (often referred to as sklearn) provides a wide array of statistical models and machine learning. sklearn, unlike most modules, is written in Python and not in C. Although it is written in Python, sklearn’s performance is attributed to its usage of NumPy for high-performance linear algebra and array operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e873ccc",
   "metadata": {},
   "source": [
    "## Installing the Scikit-Learn library\n",
    "\n",
    "Scikit-Learn requires the following libraries to be pre-installed: NumPy, SciPy, Matplotlib, IPython, Sympy, and Pandas.\n",
    "\n",
    ">1. pip install numpy\n",
    "2. pip install scipy\n",
    "3. pip install matplotlib\n",
    "4. pip install ipython\n",
    "5. pip install sympy\n",
    "6. pip install pandas\n",
    "\n",
    "Now that we’ve installed the dependent libraries let us install Scikit-Learn.\n",
    "\n",
    "> pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1d6df",
   "metadata": {},
   "source": [
    "# Application 1. Scikit-Learn for standardization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c5eff",
   "metadata": {},
   "source": [
    "### Why do we need standardization? \n",
    "\n",
    "Distance based models are machine learning algorithms that use distances to check if two data points are similar or not. If two points are close together, one can infer that the feature values are simiar and hence, can be classified as similar. \n",
    "\n",
    "Standardization is an essential task for distance based models so that one particular feature does not dominate over the other.\n",
    "\n",
    "### How to standairze our data? \n",
    "A data point $x$ is standarized as follows: \n",
    "\n",
    "$$ Z = \\frac{x - \\mu}{\\sigma} $$ \n",
    "where $\\mu$ is the mean of the distribution and $\\sigma$ is the standard deviation of the distribution. \n",
    "\n",
    "After standardization, all the data points are between $-1$ and $1$. Its mean is $0$ and its standard deviation is $1$. \n",
    "\n",
    "### Example of standarization with Scikit-learn\n",
    "\n",
    "\n",
    "Suppose temperatures recorded in Bloomington (in Fahrenheits) in Illinois in month of January are: \n",
    "\n",
    "temperatures_list = [33.2,33.1,33.1,33.0,32.9,32.9,32.8,32.8,32.7,32.7,32.6,32.6,32.6,32.6,\n",
    "                    32.5,32.5,32.5,32.6,32.6,32.6,32.7,32.7,32.8,32.9,33.0,33.1,33.2,33.4,33.5, 33.7, 33.9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a6d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2863a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of temperatures recorded in Bloomington (in Fahrenheits) in Illinois in month of January\n",
    "temperatures_list = [33.2,33.1,33.1,33.0,32.9,32.9,32.8,32.8,32.7,32.7,32.6,32.6,32.6,32.6,\n",
    "                    32.5,32.5,32.5,32.6,32.6,32.6,32.7,32.7,32.8,32.9,33.0,33.1,33.2,33.4,33.5, 33.7, 33.9]\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "temperatures_np = np.array(temperatures_list).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a32608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Before Standardizing: 32.896774193548396\n",
      "Mean After Standardizing: -2.6215588839535955e-15\n"
     ]
    }
   ],
   "source": [
    "# Standardize the vector\n",
    "temperatures_std = StandardScaler().fit_transform(temperatures_np)\n",
    "\n",
    "# Print the means\n",
    "print(\"Mean Before Standardizing:\",sum(temperatures_list)/len(temperatures_list))\n",
    "print(\"Mean After Standardizing:\",sum(temperatures_std.reshape(1,-1)[0])/len(temperatures_std))\n",
    "\n",
    "# Output:\n",
    "# Mean Before Standardizing: 32.896774193548396\n",
    "# Mean After Standardizing: -2.6215588839535955e-15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd914896",
   "metadata": {},
   "source": [
    "# Application 2. Scikit-Learn for normalization\n",
    "\n",
    "### Why do we need normalization? \n",
    "Normalization is another feature scaling technique used to transform the values of the numeric attributes to a standard scale (0 to 1). Normalization is used in cases where the values do not follow Gaussian distribution. \n",
    "\n",
    "#### Rule of thumb - Standardize if the attribute can be modeled to be a Gaussian distribution. If not, normalize).\n",
    "\n",
    "Normalization is important because it does not provide a window for the model to prefer one attribute because of the scale of values.\n",
    "\n",
    "### How to normalize our data? \n",
    "A data point $x$ is normalized as follows:\n",
    "\n",
    "$$ x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "### Example of normalization with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "226f1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "#List of temperatures recorded in Bloomington\n",
    "temperatures_list = [33.2,33.1,33.1,33.0,32.9,32.9,32.8,32.8,32.7,32.7,32.6,32.6,32.6,32.6,\n",
    "                    32.5,32.5,32.5,32.6,32.6,32.6,32.7,32.7,32.8,32.9,33.0,33.1,33.2,33.4,33.5, 33.7, 33.9]\n",
    "\n",
    "#Convert the list to a NumPy array\n",
    "temperatures_np = np.array(temperatures_list).reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9256f379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Value Before Normalization: 32.5\n",
      "Maximum Value Before Normalization: 33.9\n",
      "Minimum Value After Normalization: [0.]\n",
      "Maximum Value After Normalization: [1.]\n"
     ]
    }
   ],
   "source": [
    "#Normalize the vector\n",
    "temperatures_norm = MinMaxScaler().fit_transform(temperatures_np)\n",
    "\n",
    "print(\"Minimum Value Before Normalization:\",min(temperatures_np.reshape(1,-1)[0]))\n",
    "print(\"Maximum Value Before Normalization:\",max(temperatures_np.reshape(1,-1)[0]))\n",
    "print(\"Minimum Value After Normalization:\",min(temperatures_norm))\n",
    "print(\"Maximum Value After Normalization:\",max(temperatures_norm))\n",
    "\n",
    "# Output:\n",
    "# Minimum Value Before Normalization: 32.5\n",
    "# Maximum Value Before Normalization: 33.9\n",
    "# Minimum Value After Normalization: [0.]\n",
    "# Maximum Value After Normalization: [1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357827e",
   "metadata": {},
   "source": [
    "# Application 3. Scikit-Learn when encoding categorical features\n",
    "\n",
    "### Why do we need to encode categorical features? \n",
    "Almost every dataset has a feature (or more than one feature), that is categorical. For example, consider a dataset containing the details of all the passengers of a certain airline. The possible categorical variables in the dataset could be the passenger’s gender (male/female) and their seating choice (economy, business, first-class). All values have be numerical data for modelling, and hence, these categorical features have to be encoded.\n",
    "\n",
    "### 2 types of encoding - Label Encoding and One Hot Encoding\n",
    "\n",
    "Consider we have a list of customer's seating choices: business, economy, first-class. \n",
    "\n",
    "    Label Encoding would mean replacing all “business” with 0, all “economy” with 1, and all “first-class” with 2. \n",
    "\n",
    "    One hot encoding would have three features, 1 representing if the customer has indeed the seating choice, 0 indicating otherwise.\n",
    "\n",
    "\n",
    "### Example of Label-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74345c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# label encoder will automatically consider: \n",
    "# business = 0, economy = 1, and first-class = 2 \n",
    "\n",
    "seating = ['economy', 'business', 'first-class', 'business']\n",
    "\n",
    "# Invoking an instance of Label Encoder\n",
    "label_encoding = LabelEncoder()\n",
    "\n",
    "# Fit the labels\n",
    "encoded = label_encoding.fit(seating)\n",
    "\n",
    "print(encoded.transform(seating))\n",
    "\n",
    "# Output - [1 0 2 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbdb061",
   "metadata": {},
   "source": [
    "If one were to look at the output, they would understand that the feature has been encoded. But these numbers do not make any sense. Luckily, .classes_ help us interpret what these labels are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f6bfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 business\n",
      "1 economy\n",
      "2 first-class\n"
     ]
    }
   ],
   "source": [
    "#Iterate through the classes_ list and print them\n",
    "seating_list = encoded.classes_\n",
    "\n",
    "for seating_number in range(len(seating_list)):\n",
    "    print(seating_number, seating_list[seating_number])\n",
    "\n",
    "# Output\n",
    "# 1 business \n",
    "# 2 economy \n",
    "# 3 first-class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194a505",
   "metadata": {},
   "source": [
    "### Example of one-hot-Encoding\n",
    "\n",
    "If the seating_list feature is one-hot encoded, it would be represented in 1’s and 0’s instead of decimals.\n",
    "\n",
    "for example, \n",
    "\n",
    "    business    = [1. 0. 0.] \n",
    "    economy     = [0. 1. 0.]\n",
    "    first-class = [0. 0. 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd3ced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "seating = ['economy', 'business', 'first-class', 'business']\n",
    "\n",
    "# you have to convert the list into array in order to use one-hot encoder \n",
    "seating_numpy_array = np.array(seating).reshape(-1,1)\n",
    "\n",
    "# Invoking an instance of Label Encoder\n",
    "label_encoding = OneHotEncoder()\n",
    "\n",
    "# Fit the labels\n",
    "encoded = label_encoding.fit(seating_numpy_array)\n",
    "\n",
    "print(encoded.transform(seating_numpy_array).toarray())\n",
    "\n",
    "# Output\n",
    "# [[0. 1. 0.]\n",
    "#  [1. 0. 0.]\n",
    "#  [0. 0. 1.]\n",
    "#  [1. 0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f148a",
   "metadata": {},
   "source": [
    "# Application 4. Scikit-Learn filling missing values\n",
    "\n",
    "### Why do we need scikit-learn to fill missing values? \n",
    "\n",
    "Almost 70% of time and resources are spent on collecting and cleaning the dataset for every project. When one deals with a real-life dataset, there are always missing values. Cleaning the dataset and handling missing data is important as many machine learning algorithms do not accommodate a missing attribute in the data.\n",
    "\n",
    "### Methods of dealing with missing values \n",
    "\n",
    "    1. remove the row of data with a missing value, that would mean losing valuable-yet-incomplete data. \n",
    "    2. replace the missing values with values that can be inferred from known data. \n",
    "    3. replace the missing data with the mean of that column.\n",
    "\n",
    "Missing values are encoded with NumPy’s NaN (numpy.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce9a1e",
   "metadata": {},
   "source": [
    "### Example of using Scikit-learn to fill missing values \n",
    "\n",
    "Suppose the temperatures recorded in Bloomington (in Fahrenheits) in Illinois in the month of February:\n",
    "    \n",
    "temperatures = [33.2,32.8,32.9,33.0,\"NaN\",33.2,33.4,33.1,32.6,32.5,32.5,33.1,33.0,\"NaN\",32.7,32.7,32.6,\"NaN\",32.6,32.9,32.8,\n",
    "                32.8,32.5,32.6,\"NaN\",32.6,32.7,32.7,33.5, 33.7,33.9]\n",
    "    \n",
    "##### Let’s try to replace the missing temperatures with their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2fb60d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.2       , 32.8       , 32.9       , 33.        , 32.91111111,\n",
       "        33.2       , 33.4       , 33.1       , 32.6       , 32.5       ,\n",
       "        32.5       , 33.1       , 33.        , 32.91111111, 32.7       ,\n",
       "        32.7       , 32.6       , 32.91111111, 32.6       , 32.9       ,\n",
       "        32.8       , 32.8       , 32.5       , 32.6       , 32.91111111,\n",
       "        32.6       , 32.7       , 32.7       , 33.5       , 33.7       ,\n",
       "        33.9       ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#List of temperatures\n",
    "temperatures = [33.2,32.8,32.9,33.0,\"NaN\",33.2,33.4,33.1,32.6,32.5,32.5,33.1,33.0,\"NaN\",32.7,32.7,32.6,\"NaN\",32.6,32.9,32.8,\n",
    "                32.8,32.5,32.6,\"NaN\",32.6,32.7,32.7,33.5, 33.7,33.9]\n",
    "\n",
    "temperatures_cleaned = []\n",
    "\n",
    "#Replace NaN's with np.nan\n",
    "for temperature in temperatures:\n",
    "    if temperature==\"NaN\":\n",
    "        temperatures_cleaned.append(np.nan)\n",
    "    else:\n",
    "        temperatures_cleaned.append(temperature)\n",
    "        \n",
    "# convert the list into an array and reshape it \n",
    "temperatures_np = np.array(temperatures_cleaned).reshape(-1,1)\n",
    "\n",
    "# Create an instance of the imputer\n",
    "imputer_mean = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "\n",
    "#Transform the array and fit according to the chosen strategy\n",
    "temperatures_np = imputer_mean.fit_transform(temperatures_np)\n",
    "\n",
    "# reshape the output array \n",
    "temperatures_np.reshape(1,len(temperatures_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba414f9",
   "metadata": {},
   "source": [
    "### SimpleImputer provides four options for strategy \n",
    "\n",
    "    - mean, median, most_frequent, and constant. \n",
    "    \n",
    "    Since mean was the chosen strategy, the nan’s were replaced with the mean of the temperatures (32.91111111).\n",
    "\n",
    "##### Let’s try to replace the missing temperatures with their most_frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb9557c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.2, 32.8, 32.9, 33. , 32.6, 33.2, 33.4, 33.1, 32.6, 32.5, 32.5,\n",
       "        33.1, 33. , 32.6, 32.7, 32.7, 32.6, 32.6, 32.6, 32.9, 32.8, 32.8,\n",
       "        32.5, 32.6, 32.6, 32.6, 32.7, 32.7, 33.5, 33.7, 33.9]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of temperatures\n",
    "temperatures = [33.2,32.8,32.9,33.0,\"NaN\",33.2,33.4,33.1,32.6,32.5,32.5,33.1,33.0,\"NaN\",32.7,32.7,32.6,\"NaN\",32.6,32.9,32.8,\n",
    "                32.8,32.5,32.6,\"NaN\",32.6,32.7,32.7,33.5, 33.7,33.9]\n",
    "\n",
    "temperatures_cleaned = []\n",
    "\n",
    "#Replace NaN's with np.nan\n",
    "for temperature in temperatures:\n",
    "    if temperature==\"NaN\":\n",
    "        temperatures_cleaned.append(np.nan)\n",
    "    else:\n",
    "        temperatures_cleaned.append(temperature)\n",
    "        \n",
    "# convert the list into an array and reshape it \n",
    "temperatures_np = np.array(temperatures_cleaned).reshape(-1,1)\n",
    "\n",
    "# Create an instance of the imputer\n",
    "imputer_most_frequent = SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
    "\n",
    "#Transform the array and fit according to the chosen strategy\n",
    "temperatures_np = imputer_most_frequent.fit_transform(temperatures_np)\n",
    "\n",
    "# reshape the output array \n",
    "temperatures_np.reshape(1,len(temperatures_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b958b1",
   "metadata": {},
   "source": [
    "##### Let’s try to replace the missing temperatures with a constant 32.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19171ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.2, 32.8, 32.9, 33. , 32.9, 33.2, 33.4, 33.1, 32.6, 32.5, 32.5,\n",
       "        33.1, 33. , 32.9, 32.7, 32.7, 32.6, 32.9, 32.6, 32.9, 32.8, 32.8,\n",
       "        32.5, 32.6, 32.9, 32.6, 32.7, 32.7, 33.5, 33.7, 33.9]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of temperatures\n",
    "temperatures = [33.2,32.8,32.9,33.0,\"NaN\",33.2,33.4,33.1,32.6,32.5,32.5,33.1,33.0,\"NaN\",32.7,32.7,32.6,\"NaN\",32.6,32.9,32.8,\n",
    "                32.8,32.5,32.6,\"NaN\",32.6,32.7,32.7,33.5, 33.7,33.9]\n",
    "\n",
    "temperatures_cleaned = []\n",
    "\n",
    "#Replace NaN's with np.nan\n",
    "for temperature in temperatures:\n",
    "    if temperature==\"NaN\":\n",
    "        temperatures_cleaned.append(np.nan)\n",
    "    else:\n",
    "        temperatures_cleaned.append(temperature)\n",
    "        \n",
    "# convert the list into an array and reshape it \n",
    "temperatures_np = np.array(temperatures_cleaned).reshape(-1,1)\n",
    "\n",
    "# Create an instance of the imputer\n",
    "imputer_constant = SimpleImputer(missing_values=np.nan,strategy='constant',fill_value=32.9)\n",
    "\n",
    "#Transform the array and fit according to the chosen strategy\n",
    "temperatures_np = imputer_constant.fit_transform(temperatures_np)\n",
    "\n",
    "# reshape the output array \n",
    "temperatures_np.reshape(1,len(temperatures_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bf9e5",
   "metadata": {},
   "source": [
    "# Summary: \n",
    "\n",
    "After going over this notebook, you shall be able to use Scikit-learn to do the followingS: \n",
    "\n",
    "1. standardization \n",
    "2. normalization \n",
    "3. encoding categorical features\n",
    "4. filling missing values\n",
    "\n",
    "# Next: scikit-learn part 2 for machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65602518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
